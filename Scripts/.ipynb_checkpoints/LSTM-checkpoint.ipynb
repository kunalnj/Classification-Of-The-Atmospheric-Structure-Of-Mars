{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "from peakfinder import detect_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn lstm model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\smooth_labels.dat',delimiter = '\\s+',header = None,index_col = None)\n",
    "#df_labels = pd.read_csv(r'/Users/kunal/OneDrive - The Open University/SPIN/Transmission/Scripts/smooth_labels.dat',delimiter = '\\s+',header = None,index_col = None)\n",
    "#df_labels = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\encoded labels.txt',delimiter = '\\s+',header = None,index_col = None)\n",
    "#df_labels = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\modified_labels1.txt',delimiter = '\\s+',header = None,index_col = None)\n",
    "#df_labels = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\new labels1.txt',delimiter = '\\s+',header = None,index_col = None)\n",
    "\n",
    "df_labels.columns = ['File','Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97\n",
       "1    71\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "y = []\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "f_wav = 250\n",
    "\n",
    "rootDir = r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\data\\level_1p0_data\\New occultations'\n",
    "#rootDir = r'/Users/kunal/OneDrive - The Open University/SPIN/data/level_1p0_data/New occultations'\n",
    "os.chdir(rootDir)\n",
    "list_of_files = os.listdir(os.getcwd())\n",
    "\n",
    "for each_df_file in df_labels['File']:\n",
    "    for each_file in list_of_files:\n",
    "        if each_file.startswith(each_df_file):\n",
    "            \n",
    "            file = h5py.File(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\data\\level_1p0_data\\New occultations\\%s' %each_file,'r')\n",
    "            #file = h5py.File(r'/Users/kunal/OneDrive - The Open University/SPIN/data/level_1p0_data/New occultations/%s' %each_file,'r')\n",
    "\n",
    "            \n",
    "            T = np.array(file['Science/Transmission'])\n",
    "            TangAlt = np.array(file['Geometry/Point0/TangentAltSurface'])\n",
    "            wav = np.array(file['Science/Wavelength'])\n",
    "\n",
    "            avg_TangAlt = []\n",
    "\n",
    "            for j in range(TangAlt.shape[0]):\n",
    "                avg_TangAlt.append(np.mean(TangAlt[j,:]))\n",
    "\n",
    "\n",
    "\n",
    "            T_wav = T[:,np.array(np.where(wav == find_nearest(wav,f_wav))).flatten()].reshape(-1,)\n",
    "\n",
    "            if T_wav[0] > 0.5:\n",
    "                T_wav = T_wav[::-1]\n",
    "\n",
    "            z.append(T_wav)\n",
    "            \n",
    "            if avg_TangAlt[0] > 100:\n",
    "                avg_TangAlt = avg_TangAlt[::-1]\n",
    "            \n",
    "            y.append(avg_TangAlt)\n",
    "            \n",
    "z = np.array(z)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File     20181202_212602\n",
      "Label                  0\n",
      "Name: 21, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z[21])\n",
    "print(df_labels.iloc[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_false_files_encoded(show_files):\n",
    "    \n",
    "    false_files = []\n",
    "    index = []\n",
    "    diff = np.array(preds - testy)\n",
    "    \n",
    "    if show_files:\n",
    "        for i,_ in enumerate(diff):\n",
    "            if np.array_equal(_,np.array([0.,0.,0.,0.])) == False:\n",
    "                false_files.append(df_labels['File'][id_test[i]])\n",
    "                index.append(id_test[i])\n",
    "                \n",
    "        df_false = pd.DataFrame(columns=['Filename','True Label','Predicted Label'],index=index)\n",
    "        df_false['Filename'] = false_files\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        \n",
    "        for j,_ in enumerate(np.array(preds - testy)):\n",
    "            if np.array_equal(_,np.array([0.,0.,0.,0.])) == False:\n",
    "                a.append(testy[j])\n",
    "                b.append(preds[j])\n",
    "\n",
    "        df_false['True Label'] = a\n",
    "        df_false['Predicted Label'] = b\n",
    "        df_false.sort_index(inplace = True)\n",
    "        \n",
    "\n",
    "        os.chdir(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Plot_%snm' %f_wav)\n",
    "        file_list = os.listdir()\n",
    "        for each_file in file_list:\n",
    "            for _ in false_files:\n",
    "                if each_file.startswith(_):\n",
    "                    img = Image.open(each_file)\n",
    "                    img.show()\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        for i,_ in enumerate(diff):\n",
    "            if np.array_equal(_,np.array([0.,0.,0.,0.])) == False:\n",
    "                false_files.append(df_labels['File'][id_test[i]])\n",
    "                index.append(id_test[i])\n",
    "                \n",
    "        df_false = pd.DataFrame(columns=['Filename','True Label','Predicted Label'],index=index)\n",
    "        df_false['Filename'] = false_files\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        \n",
    "        for j,_ in enumerate(np.array(preds - testy)):\n",
    "            if np.array_equal(_,np.array([0.,0.,0.,0.])) == False:\n",
    "                a.append(testy[j])\n",
    "                b.append(preds[j])\n",
    "\n",
    "        df_false['True Label'] = a\n",
    "        df_false['Predicted Label'] = b\n",
    "        df_false.sort_index(inplace = True)\n",
    "        \n",
    "    return df_false\n",
    "\n",
    "\n",
    "def show_false_files(show_files):\n",
    "    \n",
    "    if show_files:\n",
    "        \n",
    "        false_files = df_labels['File'][id_test[np.where(abs(pred_classes-testy1) != 0)]].values\n",
    "        df_false = pd.DataFrame(columns=['Filename','True Label','Predicted Label'],index=id_test[np.where(abs(pred_classes-testy1) != 0)])\n",
    "        df_false['Filename'] = false_files\n",
    "        a = []\n",
    "        b = []\n",
    "        for i in range(len(testy1)):\n",
    "            if abs(pred_classes[i]-testy1[i]) != 0:\n",
    "                a.append(testy1[i])\n",
    "                b.append(pred_classes[i])\n",
    "\n",
    "        df_false['True Label'] = a\n",
    "        df_false['Predicted Label'] = b\n",
    "        df_false.sort_index(inplace = True)\n",
    "        \n",
    "        if show_files:\n",
    "            os.chdir(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Plot_%snm' %f_wav)\n",
    "        file_list = os.listdir()\n",
    "        for each_file in file_list:\n",
    "            for _ in false_files:\n",
    "                if each_file.startswith(_):\n",
    "                    img = Image.open(each_file)\n",
    "                    img.show()\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        false_files = df_labels['File'][id_test[np.where(abs(pred_classes-testy1) != 0)]].values\n",
    "        df_false = pd.DataFrame(columns=['Filename','True Label','Predicted Label'],index=id_test[np.where(abs(pred_classes-testy1) != 0)])\n",
    "        df_false['Filename'] = false_files\n",
    "        a = []\n",
    "        b = []\n",
    "        for i in range(len(testy1)):\n",
    "            if abs(pred_classes[i]-testy1[i]) != 0:\n",
    "                a.append(testy1[i])\n",
    "                b.append(pred_classes[i])\n",
    "\n",
    "        df_false['True Label'] = a\n",
    "        df_false['Predicted Label'] = b\n",
    "        df_false.sort_index(inplace = True)\n",
    "        \n",
    "    return df_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_interp = np.arange(0,274,1)\n",
    "t_interp = []\n",
    "for i in range(len(z)):\n",
    "    z_interp = np.interp(alt_interp,y[i],z[i])\n",
    "    t_interp.append(z_interp)\n",
    "t_interp = np.array(t_interp)\n",
    "t_interp[t_interp < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 274)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_interp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_clipped = []\n",
    "for j,t in enumerate(t_interp):\n",
    "    clip = np.array([])\n",
    "    peak_idx = detect_peaks(t)\n",
    "    for _ in t[peak_idx]:\n",
    "        clip = np.append(clip,_)\n",
    "        if _ > 0.98:\n",
    "            break\n",
    "    t_clipped.append(t[:np.where(t == clip[-1])[0][0]])\n",
    "t_clipped = np.array(t_clipped)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_len = []\n",
    "for t in t_clipped: \n",
    "    clipped_len.append(len(t))\n",
    "\n",
    "t_clipped_max = t_interp[:,:max(clipped_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,t in enumerate(t_clipped_max):\n",
    "    valley_idx = detect_peaks(t,mph=0.005,mpd=5)\n",
    "    for _ in valley_idx:\n",
    "        if 0 <= _ <= 20:\n",
    "            t_clipped_max[i][_]=5\n",
    "            break\n",
    "        if 20 < _ <= 40:\n",
    "            t_clipped_max[i][_]=10\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_interp_shaped = []\n",
    "\n",
    "for _ in range(t_interp.shape[1]):\n",
    "    t_interp_shaped.append(t_interp[:,_])\n",
    "\n",
    "t_interp_shaped = np.array(t_interp_shaped)   \n",
    "\n",
    "t_interp_shaped = t_interp.reshape(t_interp_shaped.shape[1],t_interp_shaped.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_clipped_max_shaped = []\n",
    "\n",
    "for _ in range(t_clipped_max.shape[1]):\n",
    "    t_clipped_max_shaped.append(t_clipped_max[:,_])\n",
    "\n",
    "t_clipped_max_shaped = np.array(t_clipped_max_shaped)  \n",
    "\n",
    "t_clipped_max_shaped = t_clipped_max.reshape(t_clipped_max_shaped.shape[1],t_clipped_max_shaped.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_interp = np.arange(0,274,1)\n",
    "t_interp = []\n",
    "for i in range(len(z)):\n",
    "    z_interp = np.interp(alt_interp,y[i],z[i])\n",
    "    t_interp.append(z_interp)\n",
    "t_interp = np.array(t_interp)\n",
    "t_interp[t_interp < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(np.array(df_labels['Label'])))\n",
    "#trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_clipped_max_shaped, np.array(df_labels.Label),indices, test_size=0.33,random_state = 10)\n",
    "trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_interp_shaped, np.array(df_labels.Label),indices, test_size=0.33,random_state = 10)\n",
    "\n",
    "trainy = to_categorical(trainy1)\n",
    "testy = to_categorical(testy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22971c88e80>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"indices = np.arange(len(np.array(df_labels['Label'])))\\n\\n#trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_interp_shaped, np.array(df_labels.Label.apply(ast.literal_eval)),indices, test_size=0.33)#,random_state = 10)\\ntrainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_clipped_max_shaped, np.array(df_labels.Label.apply(ast.literal_eval)),indices, test_size=0.33,random_state = 10)\\n\\ntrainy = np.zeros((len(trainy1),4))\\ntesty = np.zeros((len(testy1),4))\\n\\nfor i in range(len(trainy1)):\\n    for j in range(len(trainy1[i])):\\n        trainy[i,j] = trainy1[i][j]\\n        \\nfor i in range(len(testy1)):\\n    for j in range(len(testy1[i])):\\n        testy[i,j] = testy1[i][j]\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''indices = np.arange(len(np.array(df_labels['Label'])))\n",
    "\n",
    "#trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_interp_shaped, np.array(df_labels.Label.apply(ast.literal_eval)),indices, test_size=0.33)#,random_state = 10)\n",
    "trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_clipped_max_shaped, np.array(df_labels.Label.apply(ast.literal_eval)),indices, test_size=0.33,random_state = 10)\n",
    "\n",
    "trainy = np.zeros((len(trainy1),4))\n",
    "testy = np.zeros((len(testy1),4))\n",
    "\n",
    "for i in range(len(trainy1)):\n",
    "    for j in range(len(trainy1[i])):\n",
    "        trainy[i,j] = trainy1[i][j]\n",
    "        \n",
    "for i in range(len(testy1)):\n",
    "    for j in range(len(testy1[i])):\n",
    "        testy[i,j] = testy1[i][j]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#trainX = t_clipped_max_shaped[:120]\\n#testX = t_clipped_max_shaped[120:]\\n#trainX = t_interp_shaped[:120]\\n#testX = t_interp_shaped[120:]\\n#trainy1 = np.array(df_labels.Label[:120].apply(ast.literal_eval))\\n#testy1 = np.array(df_labels.Label[120:].apply(ast.literal_eval))\\n#trainy = np.zeros((len(trainy1),4))\\n#testy = np.zeros((len(testy1),4))\\n\\nfor i in range(len(trainy1)):\\n    for j in range(len(trainy1[i])):\\n        trainy[i,j] = trainy1[i][j]\\n        \\nfor i in range(len(testy1)):\\n    for j in range(len(testy1[i])):\\n        testy[i,j] = testy1[i][j]\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#trainX = t_clipped_max_shaped[:120]\n",
    "#testX = t_clipped_max_shaped[120:]\n",
    "#trainX = t_interp_shaped[:120]\n",
    "#testX = t_interp_shaped[120:]\n",
    "#trainy1 = np.array(df_labels.Label[:120].apply(ast.literal_eval))\n",
    "#testy1 = np.array(df_labels.Label[120:].apply(ast.literal_eval))\n",
    "#trainy = np.zeros((len(trainy1),4))\n",
    "#testy = np.zeros((len(testy1),4))\n",
    "\n",
    "for i in range(len(trainy1)):\n",
    "    for j in range(len(trainy1[i])):\n",
    "        trainy[i,j] = trainy1[i][j]\n",
    "        \n",
    "for i in range(len(testy1)):\n",
    "    for j in range(len(testy1[i])):\n",
    "        testy[i,j] = testy1[i][j]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#LSTM\\nverbose, epochs, batch_size = 1, 500, 32\\nn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\\n#class_weights = {[0,1,0,0] : 1.,[0,0,0,1] : 2., [0,0,1,0] : 3., [1,0,0,0] : 4., [0,0,1,1] : 4.}\\nclass_weights = {1 : 1.,2 : 1.5, 3 : 3,4 : 5,0 : 3}\\n\\nmodel = Sequential()\\nmodel.add(LSTM(200, input_shape=(n_timesteps,n_features)))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(200, activation='relu'))\\nmodel.add(Dense(n_outputs, activation='softmax'))\\nadam = Adam(lr = 0.001)\\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\\n# fit network\\nmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight = class_weights)\\n# evaluate model\\n#_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\\n#print(accuracy)\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#LSTM\n",
    "verbose, epochs, batch_size = 1, 500, 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "#class_weights = {[0,1,0,0] : 1.,[0,0,0,1] : 2., [0,0,1,0] : 3., [1,0,0,0] : 4., [0,0,1,1] : 4.}\n",
    "class_weights = {1 : 1.,2 : 1.5, 3 : 3,4 : 5,0 : 3}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "adam = Adam(lr = 0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight = class_weights)\n",
    "# evaluate model\n",
    "#_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "#print(accuracy)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 11:09:07.838699 17004 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0913 11:09:07.868719 17004 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0913 11:09:07.872722 17004 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0913 11:09:07.950777 17004 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0913 11:09:07.958785 17004 deprecation.py:506] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0913 11:09:07.993807 17004 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0913 11:09:08.336052 17004 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0913 11:09:08.346058 17004 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0913 11:09:08.471147 17004 deprecation.py:323] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "112/112 [==============================] - 2s 16ms/step - loss: 0.6764 - acc: 0.5982\n",
      "Epoch 2/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6776 - acc: 0.5893\n",
      "Epoch 3/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6834 - acc: 0.5893\n",
      "Epoch 4/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6682 - acc: 0.5893\n",
      "Epoch 5/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6723 - acc: 0.5893\n",
      "Epoch 6/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6822 - acc: 0.5893\n",
      "Epoch 7/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6610 - acc: 0.5893\n",
      "Epoch 8/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6709 - acc: 0.5893\n",
      "Epoch 9/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6655 - acc: 0.5893\n",
      "Epoch 10/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6572 - acc: 0.5893\n",
      "Epoch 11/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6622 - acc: 0.5893\n",
      "Epoch 12/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6634 - acc: 0.5893\n",
      "Epoch 13/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6704 - acc: 0.5893\n",
      "Epoch 14/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6661 - acc: 0.5893\n",
      "Epoch 15/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6567 - acc: 0.5893\n",
      "Epoch 16/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6592 - acc: 0.5893\n",
      "Epoch 17/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6414 - acc: 0.5893\n",
      "Epoch 18/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6426 - acc: 0.5893\n",
      "Epoch 19/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6568 - acc: 0.5893\n",
      "Epoch 20/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6541 - acc: 0.5893\n",
      "Epoch 21/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6371 - acc: 0.5982\n",
      "Epoch 22/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6351 - acc: 0.5893\n",
      "Epoch 23/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6398 - acc: 0.5982\n",
      "Epoch 24/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6179 - acc: 0.5893\n",
      "Epoch 25/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6207 - acc: 0.5893\n",
      "Epoch 26/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6038 - acc: 0.5893\n",
      "Epoch 27/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6195 - acc: 0.6161\n",
      "Epoch 28/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6235 - acc: 0.6339\n",
      "Epoch 29/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5965 - acc: 0.6786\n",
      "Epoch 30/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6175 - acc: 0.6071\n",
      "Epoch 31/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5999 - acc: 0.6250\n",
      "Epoch 32/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5924 - acc: 0.5893\n",
      "Epoch 33/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6000 - acc: 0.5804\n",
      "Epoch 34/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5884 - acc: 0.5982\n",
      "Epoch 35/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5765 - acc: 0.7411\n",
      "Epoch 36/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5936 - acc: 0.6339\n",
      "Epoch 37/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5649 - acc: 0.6875\n",
      "Epoch 38/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5784 - acc: 0.6339\n",
      "Epoch 39/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5645 - acc: 0.6964\n",
      "Epoch 40/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5571 - acc: 0.6786\n",
      "Epoch 41/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5476 - acc: 0.7321\n",
      "Epoch 42/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5490 - acc: 0.6696\n",
      "Epoch 43/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5583 - acc: 0.6964\n",
      "Epoch 44/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5591 - acc: 0.6964\n",
      "Epoch 45/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5430 - acc: 0.7143\n",
      "Epoch 46/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.5497 - acc: 0.7143\n",
      "Epoch 47/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5350 - acc: 0.7411\n",
      "Epoch 48/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.5278 - acc: 0.6964\n",
      "Epoch 49/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5225 - acc: 0.7143\n",
      "Epoch 50/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5137 - acc: 0.7500\n",
      "Epoch 51/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5327 - acc: 0.7500\n",
      "Epoch 52/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.4883 - acc: 0.7857\n",
      "Epoch 53/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5178 - acc: 0.7321\n",
      "Epoch 54/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5389 - acc: 0.7679\n",
      "Epoch 55/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5282 - acc: 0.7232\n",
      "Epoch 56/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5296 - acc: 0.7679\n",
      "Epoch 57/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5069 - acc: 0.7054\n",
      "Epoch 58/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4855 - acc: 0.7500\n",
      "Epoch 59/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5155 - acc: 0.7768\n",
      "Epoch 60/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5051 - acc: 0.7768\n",
      "Epoch 61/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5093 - acc: 0.7679\n",
      "Epoch 62/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4771 - acc: 0.7857\n",
      "Epoch 63/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4834 - acc: 0.7589\n",
      "Epoch 64/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4781 - acc: 0.7411\n",
      "Epoch 65/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4798 - acc: 0.7321\n",
      "Epoch 66/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4929 - acc: 0.7589\n",
      "Epoch 67/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.4692 - acc: 0.7500\n",
      "Epoch 68/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.4792 - acc: 0.7500\n",
      "Epoch 69/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.4943 - acc: 0.7589\n",
      "Epoch 70/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4579 - acc: 0.7679\n",
      "Epoch 71/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4857 - acc: 0.7500\n",
      "Epoch 72/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4771 - acc: 0.7054\n",
      "Epoch 73/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4581 - acc: 0.7768\n",
      "Epoch 74/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4780 - acc: 0.7500\n",
      "Epoch 75/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4531 - acc: 0.8036\n",
      "Epoch 76/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4721 - acc: 0.7589\n",
      "Epoch 77/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4778 - acc: 0.7768\n",
      "Epoch 78/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4695 - acc: 0.7768\n",
      "Epoch 79/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4948 - acc: 0.7232\n",
      "Epoch 80/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4719 - acc: 0.7946\n",
      "Epoch 81/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4617 - acc: 0.7321\n",
      "Epoch 82/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.4814 - acc: 0.7143\n",
      "Epoch 83/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4624 - acc: 0.7411\n",
      "Epoch 84/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4764 - acc: 0.7768\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 5ms/step - loss: 0.4487 - acc: 0.7589\n",
      "Epoch 86/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.4361 - acc: 0.7946\n",
      "Epoch 87/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4626 - acc: 0.7768\n",
      "Epoch 88/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4272 - acc: 0.7679\n",
      "Epoch 89/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4342 - acc: 0.7946\n",
      "Epoch 90/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4610 - acc: 0.8304\n",
      "Epoch 91/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4396 - acc: 0.7589\n",
      "Epoch 92/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4434 - acc: 0.7857\n",
      "Epoch 93/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4714 - acc: 0.7232\n",
      "Epoch 94/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4501 - acc: 0.7679\n",
      "Epoch 95/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4345 - acc: 0.8214\n",
      "Epoch 96/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4765 - acc: 0.8036\n",
      "Epoch 97/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4637 - acc: 0.7589\n",
      "Epoch 98/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4465 - acc: 0.7589\n",
      "Epoch 99/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4589 - acc: 0.7679\n",
      "Epoch 100/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4197 - acc: 0.8125\n",
      "Epoch 101/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4989 - acc: 0.7411\n",
      "Epoch 102/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4812 - acc: 0.8214\n",
      "Epoch 103/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4089 - acc: 0.8036\n",
      "Epoch 104/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4539 - acc: 0.7768\n",
      "Epoch 105/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4118 - acc: 0.8036\n",
      "Epoch 106/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4088 - acc: 0.7946\n",
      "Epoch 107/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4159 - acc: 0.8393\n",
      "Epoch 108/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4524 - acc: 0.7768\n",
      "Epoch 109/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4031 - acc: 0.8482\n",
      "Epoch 110/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4276 - acc: 0.7589\n",
      "Epoch 111/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4179 - acc: 0.8214\n",
      "Epoch 112/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4140 - acc: 0.8214\n",
      "Epoch 113/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3734 - acc: 0.8661\n",
      "Epoch 114/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3997 - acc: 0.8571\n",
      "Epoch 115/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4129 - acc: 0.8036\n",
      "Epoch 116/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4047 - acc: 0.7857\n",
      "Epoch 117/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3699 - acc: 0.8393\n",
      "Epoch 118/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4117 - acc: 0.7946\n",
      "Epoch 119/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4060 - acc: 0.7946\n",
      "Epoch 120/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3822 - acc: 0.8036\n",
      "Epoch 121/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3641 - acc: 0.8571\n",
      "Epoch 122/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3552 - acc: 0.8393\n",
      "Epoch 123/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3614 - acc: 0.8214\n",
      "Epoch 124/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.4227 - acc: 0.7768\n",
      "Epoch 125/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3655 - acc: 0.8304\n",
      "Epoch 126/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3934 - acc: 0.8304\n",
      "Epoch 127/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3318 - acc: 0.8750\n",
      "Epoch 128/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3504 - acc: 0.8750\n",
      "Epoch 129/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3819 - acc: 0.8393\n",
      "Epoch 130/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3587 - acc: 0.8929\n",
      "Epoch 131/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3494 - acc: 0.8750\n",
      "Epoch 132/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3423 - acc: 0.8661\n",
      "Epoch 133/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3482 - acc: 0.8482\n",
      "Epoch 134/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3531 - acc: 0.8750\n",
      "Epoch 135/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3509 - acc: 0.8393\n",
      "Epoch 136/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3250 - acc: 0.8750\n",
      "Epoch 137/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3291 - acc: 0.8929\n",
      "Epoch 138/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2925 - acc: 0.8929\n",
      "Epoch 139/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2950 - acc: 0.8929\n",
      "Epoch 140/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3474 - acc: 0.8571\n",
      "Epoch 141/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3219 - acc: 0.8661\n",
      "Epoch 142/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2914 - acc: 0.8839\n",
      "Epoch 143/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3177 - acc: 0.8482\n",
      "Epoch 144/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2991 - acc: 0.8839\n",
      "Epoch 145/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2866 - acc: 0.9196\n",
      "Epoch 146/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2677 - acc: 0.9107\n",
      "Epoch 147/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2373 - acc: 0.9375\n",
      "Epoch 148/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2706 - acc: 0.8750\n",
      "Epoch 149/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3397 - acc: 0.8839\n",
      "Epoch 150/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2449 - acc: 0.9196\n",
      "Epoch 151/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3366 - acc: 0.8571\n",
      "Epoch 152/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2982 - acc: 0.8750\n",
      "Epoch 153/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3010 - acc: 0.8482\n",
      "Epoch 154/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2506 - acc: 0.9464\n",
      "Epoch 155/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.3292 - acc: 0.8482\n",
      "Epoch 156/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2892 - acc: 0.9196\n",
      "Epoch 157/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2761 - acc: 0.8839\n",
      "Epoch 158/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2752 - acc: 0.9018\n",
      "Epoch 159/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.3090 - acc: 0.9018\n",
      "Epoch 160/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2602 - acc: 0.9196\n",
      "Epoch 161/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2687 - acc: 0.8929\n",
      "Epoch 162/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2386 - acc: 0.8929\n",
      "Epoch 163/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2321 - acc: 0.9107\n",
      "Epoch 164/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2586 - acc: 0.8929\n",
      "Epoch 165/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2762 - acc: 0.9196\n",
      "Epoch 166/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2513 - acc: 0.8661\n",
      "Epoch 167/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2727 - acc: 0.8929\n",
      "Epoch 168/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2642 - acc: 0.8929\n",
      "Epoch 169/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2338 - acc: 0.9107\n",
      "Epoch 170/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2533 - acc: 0.9018\n",
      "Epoch 171/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2501 - acc: 0.8839\n",
      "Epoch 172/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2021 - acc: 0.9196\n",
      "Epoch 173/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2325 - acc: 0.9196\n",
      "Epoch 174/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2311 - acc: 0.9107\n",
      "Epoch 175/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2205 - acc: 0.9196\n",
      "Epoch 176/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2108 - acc: 0.9375\n",
      "Epoch 177/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2410 - acc: 0.8839\n",
      "Epoch 178/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2151 - acc: 0.9196\n",
      "Epoch 179/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2370 - acc: 0.8929\n",
      "Epoch 180/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2026 - acc: 0.9286\n",
      "Epoch 181/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2461 - acc: 0.9196\n",
      "Epoch 182/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2344 - acc: 0.9018\n",
      "Epoch 183/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.2403 - acc: 0.9196\n",
      "Epoch 184/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2301 - acc: 0.9196\n",
      "Epoch 185/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2195 - acc: 0.9107\n",
      "Epoch 186/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1903 - acc: 0.9464\n",
      "Epoch 187/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.2191 - acc: 0.9018\n",
      "Epoch 188/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2013 - acc: 0.9196\n",
      "Epoch 189/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2006 - acc: 0.9286\n",
      "Epoch 190/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2181 - acc: 0.9286\n",
      "Epoch 191/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2066 - acc: 0.9196\n",
      "Epoch 192/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2475 - acc: 0.8839\n",
      "Epoch 193/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2006 - acc: 0.9107\n",
      "Epoch 194/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2098 - acc: 0.9464\n",
      "Epoch 195/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2122 - acc: 0.9286\n",
      "Epoch 196/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2049 - acc: 0.9107\n",
      "Epoch 197/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2539 - acc: 0.8929\n",
      "Epoch 198/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2301 - acc: 0.9196\n",
      "Epoch 199/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1862 - acc: 0.9464\n",
      "Epoch 200/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2177 - acc: 0.9286\n",
      "Epoch 201/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.2146 - acc: 0.9286\n",
      "Epoch 202/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1942 - acc: 0.9286\n",
      "Epoch 203/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2116 - acc: 0.9196\n",
      "Epoch 204/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1784 - acc: 0.9375\n",
      "Epoch 205/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2092 - acc: 0.9375\n",
      "Epoch 206/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2007 - acc: 0.9464\n",
      "Epoch 207/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1832 - acc: 0.9375\n",
      "Epoch 208/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1648 - acc: 0.9464\n",
      "Epoch 209/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2169 - acc: 0.9196\n",
      "Epoch 210/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2210 - acc: 0.9107\n",
      "Epoch 211/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2509 - acc: 0.9196\n",
      "Epoch 212/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1726 - acc: 0.9375\n",
      "Epoch 213/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1884 - acc: 0.9464\n",
      "Epoch 214/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1869 - acc: 0.9464\n",
      "Epoch 215/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1747 - acc: 0.9375\n",
      "Epoch 216/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1725 - acc: 0.9286\n",
      "Epoch 217/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1719 - acc: 0.9375\n",
      "Epoch 218/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1885 - acc: 0.9375\n",
      "Epoch 219/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1876 - acc: 0.9375\n",
      "Epoch 220/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1679 - acc: 0.9464\n",
      "Epoch 221/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1535 - acc: 0.9464\n",
      "Epoch 222/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1927 - acc: 0.9018\n",
      "Epoch 223/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1697 - acc: 0.9732\n",
      "Epoch 224/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1944 - acc: 0.9196\n",
      "Epoch 225/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1677 - acc: 0.9554\n",
      "Epoch 226/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1960 - acc: 0.9464\n",
      "Epoch 227/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1528 - acc: 0.9375\n",
      "Epoch 228/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1712 - acc: 0.9464\n",
      "Epoch 229/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1767 - acc: 0.9196\n",
      "Epoch 230/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1479 - acc: 0.9643\n",
      "Epoch 231/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1695 - acc: 0.9375\n",
      "Epoch 232/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1661 - acc: 0.9286\n",
      "Epoch 233/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1551 - acc: 0.9554\n",
      "Epoch 234/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1985 - acc: 0.9375\n",
      "Epoch 235/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1752 - acc: 0.9464\n",
      "Epoch 236/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1275 - acc: 0.9554\n",
      "Epoch 237/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1646 - acc: 0.9375\n",
      "Epoch 238/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1441 - acc: 0.9643\n",
      "Epoch 239/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1830 - acc: 0.9286\n",
      "Epoch 240/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1545 - acc: 0.9375\n",
      "Epoch 241/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1367 - acc: 0.9554\n",
      "Epoch 242/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.1314 - acc: 0.9643\n",
      "Epoch 243/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1779 - acc: 0.9375\n",
      "Epoch 244/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1438 - acc: 0.9464\n",
      "Epoch 245/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1398 - acc: 0.9554\n",
      "Epoch 246/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1485 - acc: 0.9554\n",
      "Epoch 247/500\n",
      "112/112 [==============================] - 1s 8ms/step - loss: 0.1332 - acc: 0.9464\n",
      "Epoch 248/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1460 - acc: 0.9464\n",
      "Epoch 249/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1610 - acc: 0.9375\n",
      "Epoch 250/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1559 - acc: 0.9554\n",
      "Epoch 251/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1624 - acc: 0.9286\n",
      "Epoch 252/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1299 - acc: 0.9375\n",
      "Epoch 253/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1289 - acc: 0.9643\n",
      "Epoch 254/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1437 - acc: 0.9554\n",
      "Epoch 255/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1563 - acc: 0.9375\n",
      "Epoch 256/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1481 - acc: 0.9554\n",
      "Epoch 257/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.2008 - acc: 0.9286\n",
      "Epoch 258/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1708 - acc: 0.9375\n",
      "Epoch 259/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1429 - acc: 0.9375\n",
      "Epoch 260/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1575 - acc: 0.9375\n",
      "Epoch 261/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1526 - acc: 0.9554\n",
      "Epoch 262/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1574 - acc: 0.9375\n",
      "Epoch 263/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1225 - acc: 0.9554\n",
      "Epoch 264/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1468 - acc: 0.9196\n",
      "Epoch 265/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1784 - acc: 0.9286\n",
      "Epoch 266/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1326 - acc: 0.9554\n",
      "Epoch 267/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1590 - acc: 0.9464\n",
      "Epoch 268/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1353 - acc: 0.9464\n",
      "Epoch 269/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1210 - acc: 0.9643\n",
      "Epoch 270/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1854 - acc: 0.9286\n",
      "Epoch 271/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1294 - acc: 0.9554\n",
      "Epoch 272/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1432 - acc: 0.9286\n",
      "Epoch 273/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1513 - acc: 0.9375\n",
      "Epoch 274/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1382 - acc: 0.9643\n",
      "Epoch 275/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1565 - acc: 0.9375\n",
      "Epoch 276/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1749 - acc: 0.9375\n",
      "Epoch 277/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1411 - acc: 0.9375\n",
      "Epoch 278/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1393 - acc: 0.9464\n",
      "Epoch 279/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1049 - acc: 0.9554\n",
      "Epoch 280/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1629 - acc: 0.9375\n",
      "Epoch 281/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1336 - acc: 0.9375\n",
      "Epoch 282/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1686 - acc: 0.9286\n",
      "Epoch 283/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1869 - acc: 0.9375\n",
      "Epoch 284/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1511 - acc: 0.9375\n",
      "Epoch 285/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1765 - acc: 0.9554\n",
      "Epoch 286/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1243 - acc: 0.9643\n",
      "Epoch 287/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1271 - acc: 0.9643\n",
      "Epoch 288/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1276 - acc: 0.9554\n",
      "Epoch 289/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1520 - acc: 0.9375\n",
      "Epoch 290/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1599 - acc: 0.9375\n",
      "Epoch 291/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1403 - acc: 0.9464\n",
      "Epoch 292/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1622 - acc: 0.9464\n",
      "Epoch 293/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1152 - acc: 0.9732\n",
      "Epoch 294/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1081 - acc: 0.9464\n",
      "Epoch 295/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1312 - acc: 0.9375\n",
      "Epoch 296/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1202 - acc: 0.9554\n",
      "Epoch 297/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1623 - acc: 0.9196\n",
      "Epoch 298/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0836 - acc: 0.9732\n",
      "Epoch 299/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1244 - acc: 0.9554\n",
      "Epoch 300/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1004 - acc: 0.9732\n",
      "Epoch 301/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1311 - acc: 0.9375\n",
      "Epoch 302/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1087 - acc: 0.9643\n",
      "Epoch 303/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1337 - acc: 0.9464\n",
      "Epoch 304/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1222 - acc: 0.9464\n",
      "Epoch 305/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1317 - acc: 0.9554\n",
      "Epoch 306/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1695 - acc: 0.9375\n",
      "Epoch 307/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1109 - acc: 0.9464\n",
      "Epoch 308/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0947 - acc: 0.9911\n",
      "Epoch 309/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1209 - acc: 0.9732\n",
      "Epoch 310/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1181 - acc: 0.9732\n",
      "Epoch 311/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1479 - acc: 0.9196\n",
      "Epoch 312/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1267 - acc: 0.9554\n",
      "Epoch 313/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1705 - acc: 0.9464\n",
      "Epoch 314/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1135 - acc: 0.9732\n",
      "Epoch 315/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1149 - acc: 0.9554\n",
      "Epoch 316/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1251 - acc: 0.9464\n",
      "Epoch 317/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1156 - acc: 0.9732\n",
      "Epoch 318/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0797 - acc: 0.9821\n",
      "Epoch 319/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1085 - acc: 0.9554\n",
      "Epoch 320/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0973 - acc: 0.9554\n",
      "Epoch 321/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1086 - acc: 0.9375\n",
      "Epoch 322/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1099 - acc: 0.9554\n",
      "Epoch 323/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0956 - acc: 0.9643\n",
      "Epoch 324/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0785 - acc: 0.9732\n",
      "Epoch 325/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1309 - acc: 0.9464\n",
      "Epoch 326/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1267 - acc: 0.9554\n",
      "Epoch 327/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.1134 - acc: 0.9464\n",
      "Epoch 328/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1024 - acc: 0.9554\n",
      "Epoch 329/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1242 - acc: 0.9464\n",
      "Epoch 330/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1198 - acc: 0.9464\n",
      "Epoch 331/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1248 - acc: 0.9375\n",
      "Epoch 332/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1003 - acc: 0.9643\n",
      "Epoch 333/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0900 - acc: 0.9732\n",
      "Epoch 334/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1216 - acc: 0.9375\n",
      "Epoch 335/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0889 - acc: 0.9643\n",
      "Epoch 336/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1053 - acc: 0.9643\n",
      "Epoch 337/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0806 - acc: 0.9911\n",
      "Epoch 338/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1188 - acc: 0.9464\n",
      "Epoch 339/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0999 - acc: 0.9554\n",
      "Epoch 340/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1178 - acc: 0.9643\n",
      "Epoch 341/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1297 - acc: 0.9464\n",
      "Epoch 342/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1389 - acc: 0.9464\n",
      "Epoch 343/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.1144 - acc: 0.9643\n",
      "Epoch 344/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0827 - acc: 0.9732\n",
      "Epoch 345/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0598 - acc: 0.9821\n",
      "Epoch 346/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0991 - acc: 0.9643\n",
      "Epoch 347/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1124 - acc: 0.9554\n",
      "Epoch 348/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0973 - acc: 0.9643\n",
      "Epoch 349/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1087 - acc: 0.9554\n",
      "Epoch 350/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1435 - acc: 0.9375\n",
      "Epoch 351/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1049 - acc: 0.9464\n",
      "Epoch 352/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.1189 - acc: 0.9464\n",
      "Epoch 353/500\n",
      "112/112 [==============================] - 1s 7ms/step - loss: 0.0967 - acc: 0.9732\n",
      "Epoch 354/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0939 - acc: 0.9643\n",
      "Epoch 355/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1014 - acc: 0.9554\n",
      "Epoch 356/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0975 - acc: 0.9643\n",
      "Epoch 357/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0742 - acc: 0.9821\n",
      "Epoch 358/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0805 - acc: 0.9732\n",
      "Epoch 359/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1199 - acc: 0.9643\n",
      "Epoch 360/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0897 - acc: 0.9643\n",
      "Epoch 361/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0768 - acc: 0.9732\n",
      "Epoch 362/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0869 - acc: 0.9643\n",
      "Epoch 363/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0958 - acc: 0.9554\n",
      "Epoch 364/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1077 - acc: 0.9643\n",
      "Epoch 365/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1192 - acc: 0.9554\n",
      "Epoch 366/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1253 - acc: 0.9375\n",
      "Epoch 367/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1016 - acc: 0.9554\n",
      "Epoch 368/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1378 - acc: 0.9375\n",
      "Epoch 369/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0866 - acc: 0.9643\n",
      "Epoch 370/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0771 - acc: 0.9821\n",
      "Epoch 371/500\n",
      "112/112 [==============================] - 1s 6ms/step - loss: 0.0684 - acc: 0.9732\n",
      "Epoch 372/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0990 - acc: 0.9554\n",
      "Epoch 373/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0900 - acc: 0.9643\n",
      "Epoch 374/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0811 - acc: 0.9821\n",
      "Epoch 375/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0827 - acc: 0.9643\n",
      "Epoch 376/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0747 - acc: 0.9732\n",
      "Epoch 377/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0715 - acc: 0.9732\n",
      "Epoch 378/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1089 - acc: 0.9732\n",
      "Epoch 379/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0923 - acc: 0.9643\n",
      "Epoch 380/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1124 - acc: 0.9554\n",
      "Epoch 381/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1311 - acc: 0.9554\n",
      "Epoch 382/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1002 - acc: 0.9643\n",
      "Epoch 383/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.0680 - acc: 0.9821\n",
      "Epoch 384/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0880 - acc: 0.9554\n",
      "Epoch 385/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0858 - acc: 0.9732\n",
      "Epoch 386/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1307 - acc: 0.9554\n",
      "Epoch 387/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0600 - acc: 0.9821\n",
      "Epoch 388/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.1088 - acc: 0.9643\n",
      "Epoch 389/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0699 - acc: 0.9643\n",
      "Epoch 390/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0912 - acc: 0.9554\n",
      "Epoch 391/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1096 - acc: 0.9643\n",
      "Epoch 392/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0937 - acc: 0.9643\n",
      "Epoch 393/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0780 - acc: 0.9821\n",
      "Epoch 394/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1076 - acc: 0.9375\n",
      "Epoch 395/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0811 - acc: 0.9732\n",
      "Epoch 396/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1113 - acc: 0.9464\n",
      "Epoch 397/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1120 - acc: 0.9643\n",
      "Epoch 398/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0694 - acc: 0.9732\n",
      "Epoch 399/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0920 - acc: 0.9643\n",
      "Epoch 400/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0679 - acc: 0.9821\n",
      "Epoch 401/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0690 - acc: 0.9732\n",
      "Epoch 402/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.0776 - acc: 0.9732\n",
      "Epoch 403/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0885 - acc: 0.9643\n",
      "Epoch 404/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0845 - acc: 0.9643\n",
      "Epoch 405/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0948 - acc: 0.9554\n",
      "Epoch 406/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1185 - acc: 0.9554\n",
      "Epoch 407/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1136 - acc: 0.9554\n",
      "Epoch 408/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0668 - acc: 0.9732\n",
      "Epoch 409/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0689 - acc: 0.9732\n",
      "Epoch 410/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0772 - acc: 0.9554\n",
      "Epoch 411/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0875 - acc: 0.9554\n",
      "Epoch 412/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0744 - acc: 0.9732\n",
      "Epoch 413/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0800 - acc: 0.9732\n",
      "Epoch 414/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0877 - acc: 0.9643\n",
      "Epoch 415/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0930 - acc: 0.9554\n",
      "Epoch 416/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0768 - acc: 0.9821\n",
      "Epoch 417/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0719 - acc: 0.9732\n",
      "Epoch 418/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0995 - acc: 0.9643\n",
      "Epoch 419/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1032 - acc: 0.9464\n",
      "Epoch 420/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0613 - acc: 0.9732\n",
      "Epoch 421/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1161 - acc: 0.9643\n",
      "Epoch 422/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0806 - acc: 0.9643\n",
      "Epoch 423/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1115 - acc: 0.9554\n",
      "Epoch 424/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0672 - acc: 0.9732\n",
      "Epoch 425/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0718 - acc: 0.9643\n",
      "Epoch 426/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0848 - acc: 0.9643\n",
      "Epoch 427/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1457 - acc: 0.9286\n",
      "Epoch 428/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1350 - acc: 0.9375\n",
      "Epoch 429/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1028 - acc: 0.9643\n",
      "Epoch 430/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0551 - acc: 0.9911\n",
      "Epoch 431/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1335 - acc: 0.9554\n",
      "Epoch 432/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0949 - acc: 0.9464\n",
      "Epoch 433/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0690 - acc: 0.9643\n",
      "Epoch 434/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1016 - acc: 0.9643\n",
      "Epoch 435/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0966 - acc: 0.9732\n",
      "Epoch 436/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0868 - acc: 0.9554\n",
      "Epoch 437/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0947 - acc: 0.9464\n",
      "Epoch 438/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0911 - acc: 0.9554\n",
      "Epoch 439/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1019 - acc: 0.9375\n",
      "Epoch 440/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0747 - acc: 0.9821\n",
      "Epoch 441/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0571 - acc: 0.9821\n",
      "Epoch 442/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0627 - acc: 0.9732\n",
      "Epoch 443/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0790 - acc: 0.9732\n",
      "Epoch 444/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0720 - acc: 0.9554\n",
      "Epoch 445/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0596 - acc: 0.9821\n",
      "Epoch 446/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0803 - acc: 0.9732\n",
      "Epoch 447/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.0871 - acc: 0.9554\n",
      "Epoch 448/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0704 - acc: 0.9732\n",
      "Epoch 449/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0561 - acc: 0.9732\n",
      "Epoch 450/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0898 - acc: 0.9732\n",
      "Epoch 451/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0619 - acc: 0.9732\n",
      "Epoch 452/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0912 - acc: 0.9554\n",
      "Epoch 453/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0971 - acc: 0.9643\n",
      "Epoch 454/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0766 - acc: 0.9643\n",
      "Epoch 455/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0504 - acc: 0.9911\n",
      "Epoch 456/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0903 - acc: 0.9643\n",
      "Epoch 457/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0841 - acc: 0.9554\n",
      "Epoch 458/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1169 - acc: 0.9554\n",
      "Epoch 459/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1007 - acc: 0.9554\n",
      "Epoch 460/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0670 - acc: 0.9821\n",
      "Epoch 461/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0724 - acc: 0.9732\n",
      "Epoch 462/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0681 - acc: 0.9732\n",
      "Epoch 463/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0585 - acc: 0.9643\n",
      "Epoch 464/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0989 - acc: 0.9554\n",
      "Epoch 465/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0529 - acc: 0.9821\n",
      "Epoch 466/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1126 - acc: 0.9554\n",
      "Epoch 467/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0662 - acc: 0.9732\n",
      "Epoch 468/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0777 - acc: 0.9643\n",
      "Epoch 469/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0840 - acc: 0.9643\n",
      "Epoch 470/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0720 - acc: 0.9732\n",
      "Epoch 471/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0684 - acc: 0.9732\n",
      "Epoch 472/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0799 - acc: 0.9732\n",
      "Epoch 473/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0761 - acc: 0.9732\n",
      "Epoch 474/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0429 - acc: 0.9821\n",
      "Epoch 475/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0484 - acc: 0.9732\n",
      "Epoch 476/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0477 - acc: 0.9821\n",
      "Epoch 477/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0693 - acc: 0.9554\n",
      "Epoch 478/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0677 - acc: 0.9732\n",
      "Epoch 479/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0649 - acc: 0.9732\n",
      "Epoch 480/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0470 - acc: 0.9821\n",
      "Epoch 481/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0419 - acc: 0.9911\n",
      "Epoch 482/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0707 - acc: 0.9643\n",
      "Epoch 483/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0657 - acc: 0.9732\n",
      "Epoch 484/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0638 - acc: 0.9732\n",
      "Epoch 485/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0472 - acc: 0.9911\n",
      "Epoch 486/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1204 - acc: 0.9554\n",
      "Epoch 487/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0517 - acc: 0.9732\n",
      "Epoch 488/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.1121 - acc: 0.9375\n",
      "Epoch 489/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.0772 - acc: 0.9643\n",
      "Epoch 490/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.0521 - acc: 0.9821\n",
      "Epoch 491/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0783 - acc: 0.9643\n",
      "Epoch 492/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.1285 - acc: 0.9464\n",
      "Epoch 493/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0649 - acc: 0.9732\n",
      "Epoch 494/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0889 - acc: 0.9643\n",
      "Epoch 495/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0804 - acc: 0.9732\n",
      "Epoch 496/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0575 - acc: 0.9911\n",
      "Epoch 497/500\n",
      "112/112 [==============================] - 1s 4ms/step - loss: 0.0730 - acc: 0.9821\n",
      "Epoch 498/500\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0764 - acc: 0.9732\n",
      "Epoch 499/500\n",
      "112/112 [==============================] - 1s 5ms/step - loss: 0.0533 - acc: 0.9821\n",
      "Epoch 500/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 0s 4ms/step - loss: 0.0804 - acc: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22971f09ac8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNNLSTM\n",
    "verbose, epochs, batch_size = 1, 500, 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "# reshape data into time steps of sub-sequences\n",
    "n_steps, n_length = 2, 137 #interp\n",
    "#n_steps, n_length = 1, 257 #clipped max\n",
    "trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "# define model\n",
    "#class_weights = {1 : 1.,2 : 1.5, 3 : 3,4 : 5,0 : 3}\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "adam = Adam(lr = 0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = adam, metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)#,class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"verbose, epochs, batch_size = 1, 300, 32\\nn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\\n# reshape into subsequences (samples, time steps, rows, cols, channels)\\nn_steps, n_length = 2, 137 #interp\\n#n_steps, n_length = 1, 97 #clipped max\\nclass_weights = {1 : 1.,2 : 2.8, 3 : 1.9,4 : 4.5,0 : 4.5}\\ntrainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\\ntestX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\\n# define model\\nmodel = Sequential()\\nmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\\nmodel.add(Dropout(0.5))\\nmodel.add(Flatten())\\nmodel.add(Dense(100, activation='relu'))\\nmodel.add(Dense(n_outputs, activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n# fit network\\nmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight = class_weights)\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ConvLSTM\n",
    "'''verbose, epochs, batch_size = 1, 300, 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "# reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "n_steps, n_length = 2, 137 #interp\n",
    "#n_steps, n_length = 1, 97 #clipped max\n",
    "class_weights = {1 : 1.,2 : 2.8, 3 : 1.9,4 : 4.5,0 : 4.5}\n",
    "trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight = class_weights)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.67      0.72        36\n",
      "           1       0.52      0.65      0.58        20\n",
      "\n",
      "    accuracy                           0.66        56\n",
      "   macro avg       0.65      0.66      0.65        56\n",
      "weighted avg       0.68      0.66      0.67        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_classes = model.predict_classes(testX)\n",
    "print(classification_report(pred_classes,testy1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"preds = model.predict(testX)\\ncut_off = 0.55\\npreds[preds >= cut_off] = 1\\npreds[preds < cut_off] = 0\\n\\na = 0\\nfor _ in np.array(preds - testy):\\n    if np.array_equal(_,np.array([0.,0.,0.,0.])):\\n        a+=1\\nprint('Accuracy = ',a/len(preds-testy)*100, '%')\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''preds = model.predict(testX)\n",
    "cut_off = 0.55\n",
    "preds[preds >= cut_off] = 1\n",
    "preds[preds < cut_off] = 0\n",
    "\n",
    "a = 0\n",
    "for _ in np.array(preds - testy):\n",
    "    if np.array_equal(_,np.array([0.,0.,0.,0.])):\n",
    "        a+=1\n",
    "print('Accuracy = ',a/len(preds-testy)*100, '%')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 73-74: truncated \\UXXXXXXXX escape (<ipython-input-32-3673c6ae64f3>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-32-3673c6ae64f3>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 73-74: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "'''pred_classes = model.predict_classes(testX)\n",
    "df_classes = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\modified_labels1.txt',delimiter = ' ',header = None,index_col = None)\n",
    "df_classes.columns = ['File','Label']\n",
    "print(classification_report(pred_classes,df_classes.Label[id_test]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_false_files(show_files=False)\n",
    "#show_false_files_encoded(show_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
