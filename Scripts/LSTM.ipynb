{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import h5py\n",
    "from peakfinder import detect_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn lstm model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_labels = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\smooth_labels.dat',delimiter = '\\s+',header = None,index_col = None)\n",
    "#df_labels = pd.read_csv(r'/Users/kunal/OneDrive - The Open University/SPIN/Transmission/Scripts/smooth_labels.dat',delimiter = '\\s+',header = None,index_col = None)\n",
    "#df_labels = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\encoded labels.txt',delimiter = '\\s+',header = None,index_col = None)\n",
    "#df_labels = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\modified_labels1.txt',delimiter = '\\s+',header = None,index_col = None)\n",
    "df_labels = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\new labels1.txt',delimiter = '\\s+',header = None,index_col = None)\n",
    "\n",
    "df_labels.columns = ['File','Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    154\n",
       "2.0     99\n",
       "3.0     54\n",
       "0.0     40\n",
       "4.0     28\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "y = []\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "f_wav = 250\n",
    "\n",
    "rootDir = r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\data\\level_1p0_data\\New occultations'\n",
    "#rootDir = r'/Users/kunal/OneDrive - The Open University/SPIN/data/level_1p0_data/New occultations'\n",
    "os.chdir(rootDir)\n",
    "list_of_files = os.listdir(os.getcwd())\n",
    "\n",
    "for each_df_file in df_labels['File']:\n",
    "    for each_file in list_of_files:\n",
    "        if each_file.startswith(each_df_file):\n",
    "            \n",
    "            file = h5py.File(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\data\\level_1p0_data\\New occultations\\%s' %each_file,'r')\n",
    "            #file = h5py.File(r'/Users/kunal/OneDrive - The Open University/SPIN/data/level_1p0_data/New occultations/%s' %each_file,'r')\n",
    "\n",
    "            \n",
    "            T = np.array(file['Science/Transmission'])\n",
    "            TangAlt = np.array(file['Geometry/Point0/TangentAltSurface'])\n",
    "            wav = np.array(file['Science/Wavelength'])\n",
    "\n",
    "            avg_TangAlt = []\n",
    "\n",
    "            for j in range(TangAlt.shape[0]):\n",
    "                avg_TangAlt.append(np.mean(TangAlt[j,:]))\n",
    "\n",
    "\n",
    "\n",
    "            T_wav = T[:,np.array(np.where(wav == find_nearest(wav,f_wav))).flatten()].reshape(-1,)\n",
    "\n",
    "            if T_wav[0] > 0.5:\n",
    "                T_wav = T_wav[::-1]\n",
    "\n",
    "            z.append(T_wav)\n",
    "            \n",
    "            if avg_TangAlt[0] > 100:\n",
    "                avg_TangAlt = avg_TangAlt[::-1]\n",
    "            \n",
    "            y.append(avg_TangAlt)\n",
    "            \n",
    "z = np.array(z)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File     20180504_180633_1p0_UVIS_U.h5\n",
      "Label                                2\n",
      "Name: 21, dtype: object\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z[21])\n",
    "print(df_labels.iloc[21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_false_files_encoded(show_files):\n",
    "    \n",
    "    false_files = []\n",
    "    index = []\n",
    "    diff = np.array(preds - testy)\n",
    "    \n",
    "    if show_files:\n",
    "        for i,_ in enumerate(diff):\n",
    "            if np.array_equal(_,np.array([0.,0.,0.,0.])) == False:\n",
    "                false_files.append(df_labels['File'][id_test[i]])\n",
    "                index.append(id_test[i])\n",
    "                \n",
    "        df_false = pd.DataFrame(columns=['Filename','True Label','Predicted Label'],index=index)\n",
    "        df_false['Filename'] = false_files\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        \n",
    "        for j,_ in enumerate(np.array(preds - testy)):\n",
    "            if np.array_equal(_,np.array([0.,0.,0.,0.])) == False:\n",
    "                a.append(testy[j])\n",
    "                b.append(preds[j])\n",
    "\n",
    "        df_false['True Label'] = a\n",
    "        df_false['Predicted Label'] = b\n",
    "        df_false.sort_index(inplace = True)\n",
    "        \n",
    "\n",
    "        os.chdir(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Plot_%snm' %f_wav)\n",
    "        file_list = os.listdir()\n",
    "        for each_file in file_list:\n",
    "            for _ in false_files:\n",
    "                if each_file.startswith(_):\n",
    "                    img = Image.open(each_file)\n",
    "                    img.show()\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        for i,_ in enumerate(diff):\n",
    "            if np.array_equal(_,np.array([0.,0.,0.,0.])) == False:\n",
    "                false_files.append(df_labels['File'][id_test[i]])\n",
    "                index.append(id_test[i])\n",
    "                \n",
    "        df_false = pd.DataFrame(columns=['Filename','True Label','Predicted Label'],index=index)\n",
    "        df_false['Filename'] = false_files\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        \n",
    "        for j,_ in enumerate(np.array(preds - testy)):\n",
    "            if np.array_equal(_,np.array([0.,0.,0.,0.])) == False:\n",
    "                a.append(testy[j])\n",
    "                b.append(preds[j])\n",
    "\n",
    "        df_false['True Label'] = a\n",
    "        df_false['Predicted Label'] = b\n",
    "        df_false.sort_index(inplace = True)\n",
    "        \n",
    "    return df_false\n",
    "\n",
    "\n",
    "def show_false_files(show_files):\n",
    "    \n",
    "    if show_files:\n",
    "        \n",
    "        false_files = df_labels['File'][id_test[np.where(abs(pred_classes-testy1) != 0)]].values\n",
    "        df_false = pd.DataFrame(columns=['Filename','True Label','Predicted Label'],index=id_test[np.where(abs(pred_classes-testy1) != 0)])\n",
    "        df_false['Filename'] = false_files\n",
    "        a = []\n",
    "        b = []\n",
    "        for i in range(len(testy1)):\n",
    "            if abs(pred_classes[i]-testy1[i]) != 0:\n",
    "                a.append(testy1[i])\n",
    "                b.append(pred_classes[i])\n",
    "\n",
    "        df_false['True Label'] = a\n",
    "        df_false['Predicted Label'] = b\n",
    "        df_false.sort_index(inplace = True)\n",
    "        \n",
    "        if show_files:\n",
    "            os.chdir(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Plot_%snm' %f_wav)\n",
    "        file_list = os.listdir()\n",
    "        for each_file in file_list:\n",
    "            for _ in false_files:\n",
    "                if each_file.startswith(_):\n",
    "                    img = Image.open(each_file)\n",
    "                    img.show()\n",
    "                    \n",
    "    else:\n",
    "        \n",
    "        false_files = df_labels['File'][id_test[np.where(abs(pred_classes-testy1) != 0)]].values\n",
    "        df_false = pd.DataFrame(columns=['Filename','True Label','Predicted Label'],index=id_test[np.where(abs(pred_classes-testy1) != 0)])\n",
    "        df_false['Filename'] = false_files\n",
    "        a = []\n",
    "        b = []\n",
    "        for i in range(len(testy1)):\n",
    "            if abs(pred_classes[i]-testy1[i]) != 0:\n",
    "                a.append(testy1[i])\n",
    "                b.append(pred_classes[i])\n",
    "\n",
    "        df_false['True Label'] = a\n",
    "        df_false['Predicted Label'] = b\n",
    "        df_false.sort_index(inplace = True)\n",
    "        \n",
    "    return df_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_interp = np.arange(0,274,1)\n",
    "t_interp = []\n",
    "for i in range(len(z)):\n",
    "    z_interp = np.interp(alt_interp,y[i],z[i])\n",
    "    t_interp.append(z_interp)\n",
    "t_interp = np.array(t_interp)\n",
    "t_interp[t_interp < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 274)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_interp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_clipped = []\n",
    "for j,t in enumerate(t_interp):\n",
    "    clip = np.array([])\n",
    "    peak_idx = detect_peaks(t)\n",
    "    for _ in t[peak_idx]:\n",
    "        clip = np.append(clip,_)\n",
    "        if _ > 0.98:\n",
    "            break\n",
    "    t_clipped.append(t[:np.where(t == clip[-1])[0][0]])\n",
    "t_clipped = np.array(t_clipped)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_len = []\n",
    "for t in t_clipped: \n",
    "    clipped_len.append(len(t))\n",
    "\n",
    "t_clipped_max = t_interp[:,:max(clipped_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,t in enumerate(t_clipped_max):\n",
    "    valley_idx = detect_peaks(t,mph=0.005,mpd=5)\n",
    "    for _ in valley_idx:\n",
    "        if 0 <= _ <= 20:\n",
    "            t_clipped_max[i][_]=5\n",
    "            break\n",
    "        if 20 < _ <= 40:\n",
    "            t_clipped_max[i][_]=10\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_interp_shaped = []\n",
    "\n",
    "for _ in range(t_interp.shape[1]):\n",
    "    t_interp_shaped.append(t_interp[:,_])\n",
    "\n",
    "t_interp_shaped = np.array(t_interp_shaped)   \n",
    "\n",
    "t_interp_shaped = t_interp.reshape(t_interp_shaped.shape[1],t_interp_shaped.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_clipped_max_shaped = []\n",
    "\n",
    "for _ in range(t_clipped_max.shape[1]):\n",
    "    t_clipped_max_shaped.append(t_clipped_max[:,_])\n",
    "\n",
    "t_clipped_max_shaped = np.array(t_clipped_max_shaped)  \n",
    "\n",
    "t_clipped_max_shaped = t_clipped_max.reshape(t_clipped_max_shaped.shape[1],t_clipped_max_shaped.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_interp = np.arange(0,274,1)\n",
    "t_interp = []\n",
    "for i in range(len(z)):\n",
    "    z_interp = np.interp(alt_interp,y[i],z[i])\n",
    "    t_interp.append(z_interp)\n",
    "t_interp = np.array(t_interp)\n",
    "t_interp[t_interp < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(np.array(df_labels['Label'])))\n",
    "#trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_clipped_max_shaped, np.array(df_labels.Label),indices, test_size=0.33,random_state = 10)\n",
    "trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_interp_shaped, np.array(df_labels.Label),indices, test_size=0.33,random_state = 10)\n",
    "\n",
    "trainy = to_categorical(trainy1)\n",
    "testy = to_categorical(testy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20ced5d7358>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"indices = np.arange(len(np.array(df_labels['Label'])))\\n\\n#trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_interp_shaped, np.array(df_labels.Label.apply(ast.literal_eval)),indices, test_size=0.33)#,random_state = 10)\\ntrainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_clipped_max_shaped, np.array(df_labels.Label.apply(ast.literal_eval)),indices, test_size=0.33,random_state = 10)\\n\\ntrainy = np.zeros((len(trainy1),4))\\ntesty = np.zeros((len(testy1),4))\\n\\nfor i in range(len(trainy1)):\\n    for j in range(len(trainy1[i])):\\n        trainy[i,j] = trainy1[i][j]\\n        \\nfor i in range(len(testy1)):\\n    for j in range(len(testy1[i])):\\n        testy[i,j] = testy1[i][j]\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''indices = np.arange(len(np.array(df_labels['Label'])))\n",
    "\n",
    "#trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_interp_shaped, np.array(df_labels.Label.apply(ast.literal_eval)),indices, test_size=0.33)#,random_state = 10)\n",
    "trainX, testX, trainy1, testy1,id_train,id_test = train_test_split(t_clipped_max_shaped, np.array(df_labels.Label.apply(ast.literal_eval)),indices, test_size=0.33,random_state = 10)\n",
    "\n",
    "trainy = np.zeros((len(trainy1),4))\n",
    "testy = np.zeros((len(testy1),4))\n",
    "\n",
    "for i in range(len(trainy1)):\n",
    "    for j in range(len(trainy1[i])):\n",
    "        trainy[i,j] = trainy1[i][j]\n",
    "        \n",
    "for i in range(len(testy1)):\n",
    "    for j in range(len(testy1[i])):\n",
    "        testy[i,j] = testy1[i][j]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#trainX = t_clipped_max_shaped[:120]\\n#testX = t_clipped_max_shaped[120:]\\n#trainX = t_interp_shaped[:120]\\n#testX = t_interp_shaped[120:]\\n#trainy1 = np.array(df_labels.Label[:120].apply(ast.literal_eval))\\n#testy1 = np.array(df_labels.Label[120:].apply(ast.literal_eval))\\n#trainy = np.zeros((len(trainy1),4))\\n#testy = np.zeros((len(testy1),4))\\n\\nfor i in range(len(trainy1)):\\n    for j in range(len(trainy1[i])):\\n        trainy[i,j] = trainy1[i][j]\\n        \\nfor i in range(len(testy1)):\\n    for j in range(len(testy1[i])):\\n        testy[i,j] = testy1[i][j]\\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#trainX = t_clipped_max_shaped[:120]\n",
    "#testX = t_clipped_max_shaped[120:]\n",
    "#trainX = t_interp_shaped[:120]\n",
    "#testX = t_interp_shaped[120:]\n",
    "#trainy1 = np.array(df_labels.Label[:120].apply(ast.literal_eval))\n",
    "#testy1 = np.array(df_labels.Label[120:].apply(ast.literal_eval))\n",
    "#trainy = np.zeros((len(trainy1),4))\n",
    "#testy = np.zeros((len(testy1),4))\n",
    "\n",
    "for i in range(len(trainy1)):\n",
    "    for j in range(len(trainy1[i])):\n",
    "        trainy[i,j] = trainy1[i][j]\n",
    "        \n",
    "for i in range(len(testy1)):\n",
    "    for j in range(len(testy1[i])):\n",
    "        testy[i,j] = testy1[i][j]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#LSTM\\nverbose, epochs, batch_size = 1, 500, 32\\nn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\\n#class_weights = {[0,1,0,0] : 1.,[0,0,0,1] : 2., [0,0,1,0] : 3., [1,0,0,0] : 4., [0,0,1,1] : 4.}\\nclass_weights = {1 : 1.,2 : 1.5, 3 : 3,4 : 5,0 : 3}\\n\\nmodel = Sequential()\\nmodel.add(LSTM(200, input_shape=(n_timesteps,n_features)))\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(200, activation='relu'))\\nmodel.add(Dense(n_outputs, activation='softmax'))\\nadam = Adam(lr = 0.001)\\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\\n# fit network\\nmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight = class_weights)\\n# evaluate model\\n#_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\\n#print(accuracy)\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#LSTM\n",
    "verbose, epochs, batch_size = 1, 500, 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "#class_weights = {[0,1,0,0] : 1.,[0,0,0,1] : 2., [0,0,1,0] : 3., [1,0,0,0] : 4., [0,0,1,1] : 4.}\n",
    "class_weights = {1 : 1.,2 : 1.5, 3 : 3,4 : 5,0 : 3}\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, input_shape=(n_timesteps,n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "adam = Adam(lr = 0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight = class_weights)\n",
    "# evaluate model\n",
    "#_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "#print(accuracy)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0916 15:59:06.198253 14664 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0916 15:59:06.258204 14664 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0916 15:59:06.349954 14664 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0916 15:59:06.435268 14664 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0916 15:59:06.446270 14664 deprecation.py:506] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0916 15:59:06.489301 14664 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0916 15:59:06.879101 14664 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0916 15:59:06.890110 14664 deprecation_wrapper.py:119] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0916 15:59:07.052941 14664 deprecation.py:323] From C:\\Users\\kj4755\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 1.5716 - acc: 0.3586\n",
      "Epoch 2/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.5169 - acc: 0.3705\n",
      "Epoch 3/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4874 - acc: 0.3785\n",
      "Epoch 4/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4486 - acc: 0.3944\n",
      "Epoch 5/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4777 - acc: 0.3546\n",
      "Epoch 6/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4716 - acc: 0.3745\n",
      "Epoch 7/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4777 - acc: 0.3546\n",
      "Epoch 8/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4480 - acc: 0.3785\n",
      "Epoch 9/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4540 - acc: 0.3785\n",
      "Epoch 10/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4387 - acc: 0.3825\n",
      "Epoch 11/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4560 - acc: 0.3705\n",
      "Epoch 12/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4546 - acc: 0.3865\n",
      "Epoch 13/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.4485 - acc: 0.3785\n",
      "Epoch 14/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.4277 - acc: 0.3944\n",
      "Epoch 15/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.4331 - acc: 0.3705\n",
      "Epoch 16/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4518 - acc: 0.3825\n",
      "Epoch 17/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4142 - acc: 0.4143\n",
      "Epoch 18/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4059 - acc: 0.3785\n",
      "Epoch 19/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4091 - acc: 0.3984\n",
      "Epoch 20/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.4008 - acc: 0.3745\n",
      "Epoch 21/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.3852 - acc: 0.3984\n",
      "Epoch 22/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.3744 - acc: 0.4024\n",
      "Epoch 23/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.3860 - acc: 0.4343\n",
      "Epoch 24/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.3374 - acc: 0.4223\n",
      "Epoch 25/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.3583 - acc: 0.4263\n",
      "Epoch 26/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.3420 - acc: 0.4303\n",
      "Epoch 27/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.3337 - acc: 0.4462\n",
      "Epoch 28/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.3011 - acc: 0.4741\n",
      "Epoch 29/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2831 - acc: 0.4622\n",
      "Epoch 30/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2752 - acc: 0.4701\n",
      "Epoch 31/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2641 - acc: 0.5139\n",
      "Epoch 32/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2301 - acc: 0.5339\n",
      "Epoch 33/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2345 - acc: 0.5060\n",
      "Epoch 34/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2430 - acc: 0.5538\n",
      "Epoch 35/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2326 - acc: 0.5100\n",
      "Epoch 36/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1990 - acc: 0.5139\n",
      "Epoch 37/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2338 - acc: 0.5657\n",
      "Epoch 38/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2169 - acc: 0.5139\n",
      "Epoch 39/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2023 - acc: 0.5618\n",
      "Epoch 40/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1514 - acc: 0.5458\n",
      "Epoch 41/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1487 - acc: 0.5657\n",
      "Epoch 42/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 1.1551 - acc: 0.5618\n",
      "Epoch 43/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1472 - acc: 0.5618\n",
      "Epoch 44/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.2005 - acc: 0.5538\n",
      "Epoch 45/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1579 - acc: 0.5857\n",
      "Epoch 46/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1743 - acc: 0.5777\n",
      "Epoch 47/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1428 - acc: 0.5219\n",
      "Epoch 48/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.1608 - acc: 0.5618\n",
      "Epoch 49/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.1485 - acc: 0.5458\n",
      "Epoch 50/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.1130 - acc: 0.5896\n",
      "Epoch 51/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1300 - acc: 0.5817\n",
      "Epoch 52/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1026 - acc: 0.5976\n",
      "Epoch 53/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.1514 - acc: 0.5578\n",
      "Epoch 54/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.1202 - acc: 0.5817\n",
      "Epoch 55/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1091 - acc: 0.5777\n",
      "Epoch 56/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.1469 - acc: 0.5777\n",
      "Epoch 57/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.1349 - acc: 0.5618\n",
      "Epoch 58/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0843 - acc: 0.5976\n",
      "Epoch 59/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0758 - acc: 0.5777\n",
      "Epoch 60/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0661 - acc: 0.6096\n",
      "Epoch 61/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0809 - acc: 0.5976\n",
      "Epoch 62/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0938 - acc: 0.5976\n",
      "Epoch 63/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0405 - acc: 0.5936\n",
      "Epoch 64/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0774 - acc: 0.6135\n",
      "Epoch 65/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.1130 - acc: 0.5817\n",
      "Epoch 66/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0746 - acc: 0.6056\n",
      "Epoch 67/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0634 - acc: 0.6056\n",
      "Epoch 68/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0740 - acc: 0.6016\n",
      "Epoch 69/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0593 - acc: 0.5857\n",
      "Epoch 70/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0391 - acc: 0.6135\n",
      "Epoch 71/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0702 - acc: 0.5936\n",
      "Epoch 72/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0422 - acc: 0.6255\n",
      "Epoch 73/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0300 - acc: 0.6135\n",
      "Epoch 74/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0455 - acc: 0.6016\n",
      "Epoch 75/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0117 - acc: 0.6175\n",
      "Epoch 76/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0200 - acc: 0.6175\n",
      "Epoch 77/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0163 - acc: 0.6096\n",
      "Epoch 78/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9855 - acc: 0.6335\n",
      "Epoch 79/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0092 - acc: 0.6215\n",
      "Epoch 80/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0178 - acc: 0.6096\n",
      "Epoch 81/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0308 - acc: 0.5936\n",
      "Epoch 82/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0527 - acc: 0.6056\n",
      "Epoch 83/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0405 - acc: 0.5976\n",
      "Epoch 84/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0201 - acc: 0.6096\n",
      "Epoch 85/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 1s 4ms/step - loss: 1.0053 - acc: 0.6016\n",
      "Epoch 86/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9865 - acc: 0.6016\n",
      "Epoch 87/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0119 - acc: 0.6175\n",
      "Epoch 88/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9665 - acc: 0.6295\n",
      "Epoch 89/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9993 - acc: 0.6574\n",
      "Epoch 90/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0051 - acc: 0.6255\n",
      "Epoch 91/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9807 - acc: 0.6454\n",
      "Epoch 92/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0116 - acc: 0.6135\n",
      "Epoch 93/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9987 - acc: 0.6135\n",
      "Epoch 94/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9975 - acc: 0.6135\n",
      "Epoch 95/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9657 - acc: 0.6335\n",
      "Epoch 96/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9917 - acc: 0.6016\n",
      "Epoch 97/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9638 - acc: 0.6215\n",
      "Epoch 98/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9564 - acc: 0.6175\n",
      "Epoch 99/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9614 - acc: 0.6494\n",
      "Epoch 100/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9911 - acc: 0.5976\n",
      "Epoch 101/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9572 - acc: 0.6215\n",
      "Epoch 102/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 1.0124 - acc: 0.6135\n",
      "Epoch 103/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9994 - acc: 0.6255\n",
      "Epoch 104/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9405 - acc: 0.6255\n",
      "Epoch 105/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9478 - acc: 0.6733\n",
      "Epoch 106/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9375 - acc: 0.6653\n",
      "Epoch 107/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9629 - acc: 0.6215\n",
      "Epoch 108/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9476 - acc: 0.6255\n",
      "Epoch 109/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9593 - acc: 0.6175\n",
      "Epoch 110/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9721 - acc: 0.6255\n",
      "Epoch 111/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9254 - acc: 0.6454\n",
      "Epoch 112/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9480 - acc: 0.6494\n",
      "Epoch 113/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9098 - acc: 0.6534\n",
      "Epoch 114/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9487 - acc: 0.6335\n",
      "Epoch 115/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9253 - acc: 0.6574\n",
      "Epoch 116/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9213 - acc: 0.6096\n",
      "Epoch 117/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8840 - acc: 0.6653\n",
      "Epoch 118/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9416 - acc: 0.6375\n",
      "Epoch 119/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9517 - acc: 0.6534\n",
      "Epoch 120/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8892 - acc: 0.6295\n",
      "Epoch 121/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8932 - acc: 0.6813\n",
      "Epoch 122/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8988 - acc: 0.6534\n",
      "Epoch 123/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9186 - acc: 0.6494\n",
      "Epoch 124/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8797 - acc: 0.6614\n",
      "Epoch 125/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9171 - acc: 0.6414\n",
      "Epoch 126/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8810 - acc: 0.6414\n",
      "Epoch 127/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9030 - acc: 0.6494\n",
      "Epoch 128/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9343 - acc: 0.6494\n",
      "Epoch 129/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9106 - acc: 0.6016\n",
      "Epoch 130/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9568 - acc: 0.6295\n",
      "Epoch 131/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.9347 - acc: 0.6693\n",
      "Epoch 132/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8963 - acc: 0.6534\n",
      "Epoch 133/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8996 - acc: 0.6653\n",
      "Epoch 134/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9008 - acc: 0.6653\n",
      "Epoch 135/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.9078 - acc: 0.6335\n",
      "Epoch 136/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8670 - acc: 0.6853\n",
      "Epoch 137/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8973 - acc: 0.6375\n",
      "Epoch 138/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8965 - acc: 0.6574\n",
      "Epoch 139/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8995 - acc: 0.6414\n",
      "Epoch 140/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8265 - acc: 0.6813\n",
      "Epoch 141/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8754 - acc: 0.6614\n",
      "Epoch 142/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8725 - acc: 0.6494\n",
      "Epoch 143/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8394 - acc: 0.6853\n",
      "Epoch 144/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8852 - acc: 0.6534\n",
      "Epoch 145/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8608 - acc: 0.6614\n",
      "Epoch 146/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8646 - acc: 0.6574\n",
      "Epoch 147/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8377 - acc: 0.6892\n",
      "Epoch 148/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8676 - acc: 0.6733\n",
      "Epoch 149/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8060 - acc: 0.6773\n",
      "Epoch 150/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8504 - acc: 0.6653\n",
      "Epoch 151/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8537 - acc: 0.6574\n",
      "Epoch 152/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.8760 - acc: 0.6534\n",
      "Epoch 153/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.8292 - acc: 0.7092\n",
      "Epoch 154/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8465 - acc: 0.7171\n",
      "Epoch 155/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8523 - acc: 0.6494\n",
      "Epoch 156/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8342 - acc: 0.6853\n",
      "Epoch 157/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7979 - acc: 0.7131\n",
      "Epoch 158/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8862 - acc: 0.6414\n",
      "Epoch 159/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8051 - acc: 0.6813\n",
      "Epoch 160/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8241 - acc: 0.6932\n",
      "Epoch 161/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8435 - acc: 0.6972\n",
      "Epoch 162/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8634 - acc: 0.6494\n",
      "Epoch 163/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8086 - acc: 0.6773\n",
      "Epoch 164/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8509 - acc: 0.6693\n",
      "Epoch 165/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7849 - acc: 0.7052\n",
      "Epoch 166/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8111 - acc: 0.6853\n",
      "Epoch 167/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7968 - acc: 0.7052\n",
      "Epoch 168/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 1s 5ms/step - loss: 0.7951 - acc: 0.7251\n",
      "Epoch 169/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7947 - acc: 0.6813\n",
      "Epoch 170/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8053 - acc: 0.6693\n",
      "Epoch 171/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8020 - acc: 0.6932\n",
      "Epoch 172/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8254 - acc: 0.6932\n",
      "Epoch 173/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7760 - acc: 0.7131\n",
      "Epoch 174/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8059 - acc: 0.6892\n",
      "Epoch 175/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7987 - acc: 0.7052\n",
      "Epoch 176/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8353 - acc: 0.6653\n",
      "Epoch 177/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8150 - acc: 0.6853\n",
      "Epoch 178/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.8111 - acc: 0.6693\n",
      "Epoch 179/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8132 - acc: 0.6414\n",
      "Epoch 180/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7811 - acc: 0.7092\n",
      "Epoch 181/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.8266 - acc: 0.6653\n",
      "Epoch 182/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7820 - acc: 0.6853\n",
      "Epoch 183/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7572 - acc: 0.7131\n",
      "Epoch 184/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7764 - acc: 0.6932\n",
      "Epoch 185/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7561 - acc: 0.7371\n",
      "Epoch 186/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7472 - acc: 0.7410\n",
      "Epoch 187/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7598 - acc: 0.6892\n",
      "Epoch 188/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7612 - acc: 0.6813\n",
      "Epoch 189/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7925 - acc: 0.7092\n",
      "Epoch 190/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7509 - acc: 0.7131\n",
      "Epoch 191/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7479 - acc: 0.6972\n",
      "Epoch 192/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7658 - acc: 0.7012\n",
      "Epoch 193/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7722 - acc: 0.6773\n",
      "Epoch 194/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7398 - acc: 0.7052\n",
      "Epoch 195/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7923 - acc: 0.6813\n",
      "Epoch 196/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7433 - acc: 0.7251\n",
      "Epoch 197/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7859 - acc: 0.6773\n",
      "Epoch 198/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7740 - acc: 0.7211\n",
      "Epoch 199/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7550 - acc: 0.6892\n",
      "Epoch 200/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7304 - acc: 0.6853\n",
      "Epoch 201/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7204 - acc: 0.7410\n",
      "Epoch 202/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7688 - acc: 0.6932\n",
      "Epoch 203/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7524 - acc: 0.7331\n",
      "Epoch 204/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7110 - acc: 0.7371\n",
      "Epoch 205/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7215 - acc: 0.7171\n",
      "Epoch 206/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7260 - acc: 0.7251\n",
      "Epoch 207/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7450 - acc: 0.7092\n",
      "Epoch 208/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7494 - acc: 0.6853\n",
      "Epoch 209/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7681 - acc: 0.6892\n",
      "Epoch 210/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7004 - acc: 0.7530\n",
      "Epoch 211/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7691 - acc: 0.6972\n",
      "Epoch 212/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7630 - acc: 0.6574\n",
      "Epoch 213/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7107 - acc: 0.7131\n",
      "Epoch 214/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6928 - acc: 0.6932\n",
      "Epoch 215/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7054 - acc: 0.7211\n",
      "Epoch 216/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6811 - acc: 0.7211\n",
      "Epoch 217/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6991 - acc: 0.6932\n",
      "Epoch 218/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6717 - acc: 0.7410\n",
      "Epoch 219/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7016 - acc: 0.7371\n",
      "Epoch 220/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7312 - acc: 0.7171\n",
      "Epoch 221/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7271 - acc: 0.7131\n",
      "Epoch 222/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7130 - acc: 0.7291\n",
      "Epoch 223/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7286 - acc: 0.7012\n",
      "Epoch 224/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6593 - acc: 0.7410\n",
      "Epoch 225/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7027 - acc: 0.7211\n",
      "Epoch 226/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7129 - acc: 0.7131\n",
      "Epoch 227/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7417 - acc: 0.7052\n",
      "Epoch 228/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7260 - acc: 0.7012\n",
      "Epoch 229/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6900 - acc: 0.7131\n",
      "Epoch 230/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7154 - acc: 0.6892\n",
      "Epoch 231/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7061 - acc: 0.7371\n",
      "Epoch 232/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6779 - acc: 0.7291\n",
      "Epoch 233/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6990 - acc: 0.7331\n",
      "Epoch 234/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6733 - acc: 0.7052\n",
      "Epoch 235/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6691 - acc: 0.7251\n",
      "Epoch 236/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7075 - acc: 0.6972\n",
      "Epoch 237/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6953 - acc: 0.7450\n",
      "Epoch 238/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6884 - acc: 0.6892\n",
      "Epoch 239/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6959 - acc: 0.7291\n",
      "Epoch 240/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6728 - acc: 0.7251\n",
      "Epoch 241/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6879 - acc: 0.7331\n",
      "Epoch 242/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6806 - acc: 0.7410\n",
      "Epoch 243/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7117 - acc: 0.7131\n",
      "Epoch 244/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6988 - acc: 0.7291\n",
      "Epoch 245/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6466 - acc: 0.7291\n",
      "Epoch 246/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.7009 - acc: 0.7211\n",
      "Epoch 247/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6796 - acc: 0.7052\n",
      "Epoch 248/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6741 - acc: 0.7570\n",
      "Epoch 249/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6805 - acc: 0.7410\n",
      "Epoch 250/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6574 - acc: 0.7530\n",
      "Epoch 251/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6581 - acc: 0.7291\n",
      "Epoch 252/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6996 - acc: 0.7211\n",
      "Epoch 253/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6241 - acc: 0.7450\n",
      "Epoch 254/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6270 - acc: 0.7689\n",
      "Epoch 255/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6909 - acc: 0.7371\n",
      "Epoch 256/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6766 - acc: 0.7251\n",
      "Epoch 257/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6651 - acc: 0.7211\n",
      "Epoch 258/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7120 - acc: 0.7131\n",
      "Epoch 259/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6601 - acc: 0.7371\n",
      "Epoch 260/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6686 - acc: 0.7371\n",
      "Epoch 261/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6283 - acc: 0.7849\n",
      "Epoch 262/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6420 - acc: 0.7530\n",
      "Epoch 263/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6764 - acc: 0.7171\n",
      "Epoch 264/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6059 - acc: 0.7649\n",
      "Epoch 265/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.7308 - acc: 0.7012\n",
      "Epoch 266/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6763 - acc: 0.7530\n",
      "Epoch 267/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6871 - acc: 0.7211\n",
      "Epoch 268/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6600 - acc: 0.7211\n",
      "Epoch 269/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6459 - acc: 0.7331\n",
      "Epoch 270/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6523 - acc: 0.7211\n",
      "Epoch 271/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6475 - acc: 0.7610\n",
      "Epoch 272/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6167 - acc: 0.7530\n",
      "Epoch 273/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6110 - acc: 0.7809\n",
      "Epoch 274/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6105 - acc: 0.7570\n",
      "Epoch 275/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6648 - acc: 0.7410\n",
      "Epoch 276/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6546 - acc: 0.7331\n",
      "Epoch 277/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6353 - acc: 0.7610\n",
      "Epoch 278/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6257 - acc: 0.7729\n",
      "Epoch 279/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6057 - acc: 0.7689\n",
      "Epoch 280/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6031 - acc: 0.7809\n",
      "Epoch 281/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6157 - acc: 0.7371\n",
      "Epoch 282/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6339 - acc: 0.7570\n",
      "Epoch 283/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6261 - acc: 0.7291\n",
      "Epoch 284/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.5918 - acc: 0.7729\n",
      "Epoch 285/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6844 - acc: 0.7450\n",
      "Epoch 286/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.5872 - acc: 0.7849\n",
      "Epoch 287/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6129 - acc: 0.7610\n",
      "Epoch 288/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6168 - acc: 0.7450\n",
      "Epoch 289/500\n",
      "251/251 [==============================] - 1s 6ms/step - loss: 0.6273 - acc: 0.7490\n",
      "Epoch 290/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6306 - acc: 0.7809\n",
      "Epoch 291/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.5957 - acc: 0.7729\n",
      "Epoch 292/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6048 - acc: 0.7490\n",
      "Epoch 293/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6102 - acc: 0.7649\n",
      "Epoch 294/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6142 - acc: 0.7530\n",
      "Epoch 295/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5877 - acc: 0.7689\n",
      "Epoch 296/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6539 - acc: 0.7490\n",
      "Epoch 297/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5960 - acc: 0.7769\n",
      "Epoch 298/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6050 - acc: 0.7729\n",
      "Epoch 299/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6079 - acc: 0.7610\n",
      "Epoch 300/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.5974 - acc: 0.7649\n",
      "Epoch 301/500\n",
      "251/251 [==============================] - 1s 6ms/step - loss: 0.6404 - acc: 0.7371\n",
      "Epoch 302/500\n",
      "251/251 [==============================] - 1s 6ms/step - loss: 0.5703 - acc: 0.7729\n",
      "Epoch 303/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6298 - acc: 0.7649\n",
      "Epoch 304/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.5795 - acc: 0.7769\n",
      "Epoch 305/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5969 - acc: 0.7968\n",
      "Epoch 306/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5740 - acc: 0.7968\n",
      "Epoch 307/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5603 - acc: 0.7888\n",
      "Epoch 308/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5947 - acc: 0.7888\n",
      "Epoch 309/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6047 - acc: 0.7530\n",
      "Epoch 310/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6210 - acc: 0.7769\n",
      "Epoch 311/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5675 - acc: 0.7968\n",
      "Epoch 312/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5880 - acc: 0.7809\n",
      "Epoch 313/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6141 - acc: 0.7490\n",
      "Epoch 314/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5693 - acc: 0.8088\n",
      "Epoch 315/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6001 - acc: 0.7649\n",
      "Epoch 316/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5636 - acc: 0.7610\n",
      "Epoch 317/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.6161 - acc: 0.7530\n",
      "Epoch 318/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6318 - acc: 0.7291\n",
      "Epoch 319/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5543 - acc: 0.8088\n",
      "Epoch 320/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5725 - acc: 0.7769\n",
      "Epoch 321/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5616 - acc: 0.7769\n",
      "Epoch 322/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6096 - acc: 0.7450\n",
      "Epoch 323/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6238 - acc: 0.7610\n",
      "Epoch 324/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5362 - acc: 0.7610\n",
      "Epoch 325/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5653 - acc: 0.7769\n",
      "Epoch 326/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6387 - acc: 0.7649\n",
      "Epoch 327/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6193 - acc: 0.7769\n",
      "Epoch 328/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6118 - acc: 0.7689\n",
      "Epoch 329/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5408 - acc: 0.8207\n",
      "Epoch 330/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.6121 - acc: 0.7649\n",
      "Epoch 331/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5297 - acc: 0.7888\n",
      "Epoch 332/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5820 - acc: 0.7610\n",
      "Epoch 333/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5730 - acc: 0.7849\n",
      "Epoch 334/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5750 - acc: 0.7649\n",
      "Epoch 335/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5355 - acc: 0.7968\n",
      "Epoch 336/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6005 - acc: 0.7530\n",
      "Epoch 337/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5561 - acc: 0.7849\n",
      "Epoch 338/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5867 - acc: 0.7729\n",
      "Epoch 339/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.6111 - acc: 0.7809\n",
      "Epoch 340/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5690 - acc: 0.7729\n",
      "Epoch 341/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5268 - acc: 0.7849\n",
      "Epoch 342/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5892 - acc: 0.7809\n",
      "Epoch 343/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5891 - acc: 0.7490\n",
      "Epoch 344/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5470 - acc: 0.7809\n",
      "Epoch 345/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5157 - acc: 0.8088\n",
      "Epoch 346/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5490 - acc: 0.7849\n",
      "Epoch 347/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5760 - acc: 0.7729\n",
      "Epoch 348/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5673 - acc: 0.7849\n",
      "Epoch 349/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5533 - acc: 0.7769\n",
      "Epoch 350/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5574 - acc: 0.7888\n",
      "Epoch 351/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5663 - acc: 0.7490\n",
      "Epoch 352/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5662 - acc: 0.7849\n",
      "Epoch 353/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5461 - acc: 0.8008\n",
      "Epoch 354/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5514 - acc: 0.7888\n",
      "Epoch 355/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5981 - acc: 0.7450\n",
      "Epoch 356/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5312 - acc: 0.7729\n",
      "Epoch 357/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5420 - acc: 0.7769\n",
      "Epoch 358/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5732 - acc: 0.7849\n",
      "Epoch 359/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5550 - acc: 0.7689\n",
      "Epoch 360/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5354 - acc: 0.7769\n",
      "Epoch 361/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5861 - acc: 0.7809\n",
      "Epoch 362/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4725 - acc: 0.8287\n",
      "Epoch 363/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5232 - acc: 0.8008\n",
      "Epoch 364/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5037 - acc: 0.8088\n",
      "Epoch 365/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5886 - acc: 0.7769\n",
      "Epoch 366/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5719 - acc: 0.7769\n",
      "Epoch 367/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5648 - acc: 0.7849\n",
      "Epoch 368/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5178 - acc: 0.8127\n",
      "Epoch 369/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5033 - acc: 0.7928\n",
      "Epoch 370/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5602 - acc: 0.7769\n",
      "Epoch 371/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5408 - acc: 0.7809\n",
      "Epoch 372/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5162 - acc: 0.8207\n",
      "Epoch 373/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5516 - acc: 0.7888\n",
      "Epoch 374/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5326 - acc: 0.7968\n",
      "Epoch 375/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5253 - acc: 0.8008\n",
      "Epoch 376/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5677 - acc: 0.7928\n",
      "Epoch 377/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5356 - acc: 0.7649\n",
      "Epoch 378/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5299 - acc: 0.8127\n",
      "Epoch 379/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5102 - acc: 0.8008\n",
      "Epoch 380/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4726 - acc: 0.8127\n",
      "Epoch 381/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4828 - acc: 0.8207\n",
      "Epoch 382/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5126 - acc: 0.7888\n",
      "Epoch 383/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4848 - acc: 0.8327\n",
      "Epoch 384/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4914 - acc: 0.8127\n",
      "Epoch 385/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5104 - acc: 0.8048\n",
      "Epoch 386/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5088 - acc: 0.8127\n",
      "Epoch 387/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4976 - acc: 0.8247\n",
      "Epoch 388/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5081 - acc: 0.8127\n",
      "Epoch 389/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4964 - acc: 0.8127\n",
      "Epoch 390/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5091 - acc: 0.8048\n",
      "Epoch 391/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4928 - acc: 0.8127\n",
      "Epoch 392/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4919 - acc: 0.8008\n",
      "Epoch 393/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4904 - acc: 0.8127\n",
      "Epoch 394/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4963 - acc: 0.8207\n",
      "Epoch 395/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4991 - acc: 0.7809\n",
      "Epoch 396/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.5235 - acc: 0.8048\n",
      "Epoch 397/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4670 - acc: 0.8167\n",
      "Epoch 398/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4787 - acc: 0.8127\n",
      "Epoch 399/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4897 - acc: 0.7968\n",
      "Epoch 400/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.4946 - acc: 0.8048\n",
      "Epoch 401/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4892 - acc: 0.8127\n",
      "Epoch 402/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5015 - acc: 0.8088\n",
      "Epoch 403/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.4787 - acc: 0.8247\n",
      "Epoch 404/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.4725 - acc: 0.8127\n",
      "Epoch 405/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.4788 - acc: 0.8247\n",
      "Epoch 406/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4561 - acc: 0.8566\n",
      "Epoch 407/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4799 - acc: 0.8008\n",
      "Epoch 408/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5179 - acc: 0.7769\n",
      "Epoch 409/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4837 - acc: 0.8247\n",
      "Epoch 410/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5109 - acc: 0.8088\n",
      "Epoch 411/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4799 - acc: 0.7968\n",
      "Epoch 412/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5035 - acc: 0.7928\n",
      "Epoch 413/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4672 - acc: 0.8247\n",
      "Epoch 414/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4654 - acc: 0.7968\n",
      "Epoch 415/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4815 - acc: 0.8127\n",
      "Epoch 416/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5059 - acc: 0.7968\n",
      "Epoch 417/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4866 - acc: 0.8247\n",
      "Epoch 418/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4628 - acc: 0.8287A: 0s - loss: 0.3962 - acc: \n",
      "Epoch 419/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4974 - acc: 0.8167\n",
      "Epoch 420/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.4669 - acc: 0.8127\n",
      "Epoch 421/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5114 - acc: 0.8008\n",
      "Epoch 422/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4931 - acc: 0.8088\n",
      "Epoch 423/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5000 - acc: 0.8127\n",
      "Epoch 424/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4590 - acc: 0.8446\n",
      "Epoch 425/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4611 - acc: 0.8287\n",
      "Epoch 426/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4744 - acc: 0.7968\n",
      "Epoch 427/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4590 - acc: 0.8167\n",
      "Epoch 428/500\n",
      "251/251 [==============================] - 1s 5ms/step - loss: 0.4537 - acc: 0.8247\n",
      "Epoch 429/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4785 - acc: 0.8127\n",
      "Epoch 430/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4851 - acc: 0.8127\n",
      "Epoch 431/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4601 - acc: 0.8048\n",
      "Epoch 432/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4685 - acc: 0.8088\n",
      "Epoch 433/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4272 - acc: 0.8645\n",
      "Epoch 434/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4872 - acc: 0.8167\n",
      "Epoch 435/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4291 - acc: 0.8287\n",
      "Epoch 436/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4561 - acc: 0.8327\n",
      "Epoch 437/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4359 - acc: 0.8367\n",
      "Epoch 438/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.5174 - acc: 0.7888\n",
      "Epoch 439/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4311 - acc: 0.8327\n",
      "Epoch 440/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4480 - acc: 0.8327\n",
      "Epoch 441/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4579 - acc: 0.8446\n",
      "Epoch 442/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4362 - acc: 0.8406\n",
      "Epoch 443/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4601 - acc: 0.7928\n",
      "Epoch 444/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4438 - acc: 0.8406\n",
      "Epoch 445/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4317 - acc: 0.8167\n",
      "Epoch 446/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4421 - acc: 0.7968\n",
      "Epoch 447/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4727 - acc: 0.8247\n",
      "Epoch 448/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4109 - acc: 0.8606\n",
      "Epoch 449/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4628 - acc: 0.8207\n",
      "Epoch 450/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4489 - acc: 0.8406\n",
      "Epoch 451/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4210 - acc: 0.8566\n",
      "Epoch 452/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4460 - acc: 0.8088\n",
      "Epoch 453/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4138 - acc: 0.8566\n",
      "Epoch 454/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4573 - acc: 0.8367\n",
      "Epoch 455/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.3822 - acc: 0.8486\n",
      "Epoch 456/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4090 - acc: 0.8685\n",
      "Epoch 457/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.3882 - acc: 0.8606\n",
      "Epoch 458/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4762 - acc: 0.7928\n",
      "Epoch 459/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4151 - acc: 0.8367\n",
      "Epoch 460/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4517 - acc: 0.8287\n",
      "Epoch 461/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4563 - acc: 0.8167\n",
      "Epoch 462/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4532 - acc: 0.8486\n",
      "Epoch 463/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3821 - acc: 0.8725\n",
      "Epoch 464/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4460 - acc: 0.8008\n",
      "Epoch 465/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4106 - acc: 0.8287\n",
      "Epoch 466/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4296 - acc: 0.8486\n",
      "Epoch 467/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.3919 - acc: 0.8486\n",
      "Epoch 468/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4602 - acc: 0.8207\n",
      "Epoch 469/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4168 - acc: 0.8207\n",
      "Epoch 470/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4203 - acc: 0.8526\n",
      "Epoch 471/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4616 - acc: 0.8167\n",
      "Epoch 472/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4608 - acc: 0.8207\n",
      "Epoch 473/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3982 - acc: 0.8526\n",
      "Epoch 474/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4213 - acc: 0.8805\n",
      "Epoch 475/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4065 - acc: 0.8406\n",
      "Epoch 476/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4190 - acc: 0.8287\n",
      "Epoch 477/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4340 - acc: 0.8446\n",
      "Epoch 478/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4247 - acc: 0.8327\n",
      "Epoch 479/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3775 - acc: 0.8805\n",
      "Epoch 480/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4221 - acc: 0.8367\n",
      "Epoch 481/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4126 - acc: 0.8446\n",
      "Epoch 482/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3699 - acc: 0.8645\n",
      "Epoch 483/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4101 - acc: 0.8406\n",
      "Epoch 484/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3744 - acc: 0.8606\n",
      "Epoch 485/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3986 - acc: 0.8526\n",
      "Epoch 486/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4575 - acc: 0.8088\n",
      "Epoch 487/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4218 - acc: 0.8645\n",
      "Epoch 488/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4111 - acc: 0.8845\n",
      "Epoch 489/500\n",
      "251/251 [==============================] - 1s 4ms/step - loss: 0.4106 - acc: 0.8327\n",
      "Epoch 490/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4126 - acc: 0.8446\n",
      "Epoch 491/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3809 - acc: 0.8367\n",
      "Epoch 492/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3770 - acc: 0.8486\n",
      "Epoch 493/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4359 - acc: 0.8486\n",
      "Epoch 494/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3874 - acc: 0.8645\n",
      "Epoch 495/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3995 - acc: 0.8486\n",
      "Epoch 496/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4063 - acc: 0.8446\n",
      "Epoch 497/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4048 - acc: 0.8566\n",
      "Epoch 498/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4013 - acc: 0.8566\n",
      "Epoch 499/500\n",
      "251/251 [==============================] - 1s 3ms/step - loss: 0.4386 - acc: 0.8327\n",
      "Epoch 500/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 1s 3ms/step - loss: 0.3974 - acc: 0.8406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20cee034c50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CNNLSTM\n",
    "verbose, epochs, batch_size = 1, 500, 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "# reshape data into time steps of sub-sequences\n",
    "n_steps, n_length = 2, 137 #interp\n",
    "#n_steps, n_length = 1, 257 #clipped max\n",
    "trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "# define model\n",
    "#class_weights = {1 : 1.,2 : 1.5, 3 : 3,4 : 5,0 : 3}\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "adam = Adam(lr = 0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = adam, metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)#,class_weight = class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"verbose, epochs, batch_size = 1, 300, 32\\nn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\\n# reshape into subsequences (samples, time steps, rows, cols, channels)\\nn_steps, n_length = 2, 137 #interp\\n#n_steps, n_length = 1, 97 #clipped max\\nclass_weights = {1 : 1.,2 : 2.8, 3 : 1.9,4 : 4.5,0 : 4.5}\\ntrainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\\ntestX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\\n# define model\\nmodel = Sequential()\\nmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\\nmodel.add(Dropout(0.5))\\nmodel.add(Flatten())\\nmodel.add(Dense(100, activation='relu'))\\nmodel.add(Dense(n_outputs, activation='softmax'))\\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\\n# fit network\\nmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight = class_weights)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ConvLSTM\n",
    "'''verbose, epochs, batch_size = 1, 300, 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "# reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "n_steps, n_length = 2, 137 #interp\n",
    "#n_steps, n_length = 1, 97 #clipped max\n",
    "class_weights = {1 : 1.,2 : 2.8, 3 : 1.9,4 : 4.5,0 : 4.5}\n",
    "trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight = class_weights)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.20      0.15         5\n",
      "           1       0.78      0.82      0.80        55\n",
      "           2       0.64      0.66      0.65        32\n",
      "           3       0.54      0.35      0.42        20\n",
      "           4       0.25      0.25      0.25        12\n",
      "\n",
      "    accuracy                           0.62       124\n",
      "   macro avg       0.47      0.45      0.45       124\n",
      "weighted avg       0.62      0.62      0.62       124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_classes = model.predict_classes(testX)\n",
    "print(classification_report(pred_classes,testy1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"preds = model.predict(testX)\\ncut_off = 0.55\\npreds[preds >= cut_off] = 1\\npreds[preds < cut_off] = 0\\n\\na = 0\\nfor _ in np.array(preds - testy):\\n    if np.array_equal(_,np.array([0.,0.,0.,0.])):\\n        a+=1\\nprint('Accuracy = ',a/len(preds-testy)*100, '%')\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''preds = model.predict(testX)\n",
    "cut_off = 0.55\n",
    "preds[preds >= cut_off] = 1\n",
    "preds[preds < cut_off] = 0\n",
    "\n",
    "a = 0\n",
    "for _ in np.array(preds - testy):\n",
    "    if np.array_equal(_,np.array([0.,0.,0.,0.])):\n",
    "        a+=1\n",
    "print('Accuracy = ',a/len(preds-testy)*100, '%')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 73-74: truncated \\UXXXXXXXX escape (<ipython-input-27-3673c6ae64f3>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-3673c6ae64f3>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 73-74: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "'''pred_classes = model.predict_classes(testX)\n",
    "df_classes = pd.read_csv(r'C:\\Users\\kj4755\\OneDrive - The Open University\\SPIN\\Transmission\\Scripts\\modified_labels1.txt',delimiter = ' ',header = None,index_col = None)\n",
    "df_classes.columns = ['File','Label']\n",
    "print(classification_report(pred_classes,df_classes.Label[id_test]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_false_files(show_files=False)\n",
    "#show_false_files_encoded(show_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
